{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fac71b",
   "metadata": {},
   "source": [
    "# Uncertainty on reactivity feedback coefficients using conditional expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f74798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926002d",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ae4e8",
   "metadata": {},
   "source": [
    "This method is based on the theory of conditional expectation and further develops what proposed in \"*Zwermann et al., Aleatoric and epistemic uncertainties in sampling based nuclear data uncertainty and sensitivity analyses, Proceedings of PHYSOR 2012: Conference on Advances in Reactor Physics - Linking Research, Industry, and Education; Knoxville, TN (United States); 15-20 Apr 2012*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6043b33",
   "metadata": {},
   "source": [
    "The theory is proposed for the quantification of the output uncertainty (using stochastic sampling) generated by epistemic input uncertainties, without the contribution of aleatoric uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989bb2d",
   "metadata": {},
   "source": [
    "Model:\n",
    "$$Y=f(X) + \\varepsilon$$\n",
    "where:\n",
    "- $X$ is the independent variable assumed as the only source of epistemic uncertainty: $X\\sim \\mathcal{N}(E[X], V[X])$.\n",
    "- $\\varepsilon$ is the homoscedastic aleatory uncertainty: $\\varepsilon\\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "- $Y$ is the dependent variable affected by the uncertainty on $X$ and $\\varepsilon$: $Y\\sim \\mathcal{N}(E[Y], V[Y])$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd4aaf",
   "metadata": {},
   "source": [
    "By the law of total variance:$$V[Y] = V[E[Y|X]]+E[V[Y|X]]$$\n",
    "\n",
    "The first terms on the RHS being the variance *explained* by $X$, and the second term on the RHS the *fraction of variance unexplained*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357d08e",
   "metadata": {},
   "source": [
    "The first term is the contribution of the mere epistemic uncertainty, free of the aleatoric component, i.e., what we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7560d57",
   "metadata": {},
   "source": [
    "Let's first express $V[E[Y|X]]$ for our model.\n",
    "\n",
    "$$E[Y|X]=f(X)$$\n",
    "\n",
    "$$V[E[Y|X]]=E[f(X)f(X)]$$\n",
    "\n",
    "Zwermann et al. explain an alternative approach to quantify this term as $V[E[Y|X]] = Cov(Y, Y')$. Here we need to introduce a new output variable $Y'$ as\n",
    "\n",
    "$$Y'=f(X)+\\varepsilon'$$\n",
    "\n",
    "where $\\varepsilon'$ is the homoscedastic aleatory uncertainty: $\\varepsilon'\\sim \\mathcal{N}(0, \\sigma^2)$, independent from $\\varepsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d25ca",
   "metadata": {},
   "source": [
    "Now, if $Y$ and $Y'$ are identically distributed and conditionally independent the equality $V[E[Y|X]] = Cov(Y, Y')$ holds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e707ebb",
   "metadata": {},
   "source": [
    "Let's calculate $Cov(Y, Y')$:\n",
    "\n",
    "$$Cov(Y, Y')=E[YY']=E[(f(X)+\\varepsilon)(f(X)+\\varepsilon')]=E[(f(X)f(X)]+E[f(X)\\varepsilon]+E[f(X)\\varepsilon']+E[\\varepsilon\\varepsilon']=E[(f(X)f(X)]=V[E[Y|X]]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495e21c",
   "metadata": {},
   "source": [
    "The practical implementation requires to draw a sample of $X$ with size $N$:$\\vec x=[x_1, ...,x_N]$. Then for each $i$-th observation $x_i$ we run both models and produce observations for $Y$ and $Y'$: $y_i$ and $y_i'$.\n",
    "The two vectors of observations $\\vec y$ and $\\vec y'$ have a common uncertainty source (the epsitemic one), and an independent source (the aleatoric one).\n",
    "\n",
    "The sample covariance between the two vectors provides the required information:\n",
    "\n",
    "$$Cov(Y,Y')\\approx\\frac{1}{N-1}(\\vec{y}-\\hat{\\mu}_{Y})(\\vec{y}'-\\hat{\\mu}_{Y'})^T$$\n",
    "\n",
    "with $\\hat{\\mu}_{Y}$ and $\\hat{\\mu}_{Y'}$ being the sample means of $Y$ and $Y'$ based on $N$ observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2da33",
   "metadata": {},
   "source": [
    "This concept was applied by Zwermann et al. for the quantification of the uncertainty of k$_{eff}$ due to nuclear data in Monte Carlo transport calculations. The nucelar data were identified as the epistemic source of uncertainty, while the uncertainty due to counting statistics inherent of the Monte Carlo method was the aleatoric souncertainty source.\n",
    "\n",
    "Here we extend this method for any sort of reactivity feedback coefficient estimated as\n",
    "\n",
    "$$\\Delta=k_2 - k_2$$\n",
    "\n",
    "with $k_2$ and $k_1$ being k$_{eff}$ calculations for two states of a system, e.g., for instance considering two different fuel  temperatures $T_2$ and $T_1$ to quantify the Doppler reactivity coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64fd95",
   "metadata": {},
   "source": [
    "Now, the two considered models are:\n",
    "\n",
    "$$\n",
    "\\Delta = (f_2(X) + \\varepsilon_2) - (f_1(X) + \\varepsilon_1)\n",
    "$$\n",
    "$$\n",
    "\\Delta' = (f_2(X) + \\varepsilon_2') - (f_1(X) + \\varepsilon_1')\n",
    "$$\n",
    "\n",
    "with $f_2$ and $f_1$ being two transport model repectively for temperatures $T_2$ and $T_1$, and $\\varepsilon_1$, $\\varepsilon_2$, $\\varepsilon_1'$, $\\varepsilon_2'$ being all $\\mathcal{N}(0, \\sigma^2)$ and independent. The $\\varepsilon$-terms reresent the Monte Carlo uncertainty of each simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9547faa",
   "metadata": {},
   "source": [
    "Following what previously reported, the $Cov(\\Delta, \\Delta')$ is\n",
    "\n",
    "$$\n",
    "Cov(\\Delta, \\Delta') = E[((f_2(X) + \\varepsilon_2) - (f_1(X) + \\varepsilon_1)) ((f_2(X) + \\varepsilon_2') - (f_1(X) + \\varepsilon_1'))] = E[(f_2(X)-f_1(X))^2]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "V[E[\\Delta|X]] = E[E[\\Delta|X]E[\\Delta|X]] = E[(f_2(X)-f_1(X))^2] \n",
    "$$\n",
    "\n",
    "and then $V[E[\\Delta|X]] = Cov(\\Delta, \\Delta')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eceaacd",
   "metadata": {},
   "source": [
    "## Practical implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75378e16",
   "metadata": {},
   "source": [
    "Below we report the practical implementation of the theory above for a simple problem in which we control:\n",
    "- the sample size $N$\n",
    "- the number of parameters $M$\n",
    "- the magnitude of the aleatoric uncertainty source $sigma\\_err$, and the corresponiding fraction of variance $FV$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532097e",
   "metadata": {},
   "source": [
    "Using a simple test problem we can study how the equations above converge to the analytical solution as a function of the control parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ec3ce",
   "metadata": {},
   "source": [
    "### Convergence with stat. uncertainty magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "mu = 5\n",
    "std = 2\n",
    "X = std * np.random.randn(N) + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, sigma_err=1):  # equal to f1\n",
    "    return 3 * x + sigma_err * np.random.randn(x.size)\n",
    "\n",
    "def g(x, sigma_err=1):  # equal to f2\n",
    "    return 3.1 * x + sigma_err * np.random.randn(x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274dea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "fig, ax = plt.subplots(figsize=(8, 4), dpi=100)\n",
    "\n",
    "sigma_err = .5\n",
    "FV = sigma_err**2 / (0.2**2 + sigma_err**2) * 100\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"$\\\\sigma_\\\\varepsilon={sigma_err}$, FV$={FV:.0f}$%\")\n",
    "\n",
    "sigma_err = .4\n",
    "FV = sigma_err**2 / (0.2**2 + sigma_err**2) * 100\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"$\\\\sigma_\\\\varepsilon={sigma_err}$, FV$={FV:.0f}$%\")\n",
    "\n",
    "sigma_err = .3\n",
    "FV = sigma_err**2 / (0.2**2 + sigma_err**2) * 100\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"$\\\\sigma_\\\\varepsilon={sigma_err}$, FV$={FV:.0f}$%\")\n",
    "\n",
    "sigma_err = .2\n",
    "FV = sigma_err**2 / (0.2**2 + sigma_err**2) * 100\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"$\\\\sigma_\\\\varepsilon={sigma_err}$, FV$={FV:.0f}$%\")\n",
    "\n",
    "sigma_err = .1\n",
    "FV = sigma_err**2 / (0.2**2 + sigma_err**2) * 100\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"$\\\\sigma_\\\\varepsilon={sigma_err}$, FV$={FV:.0f}$%\")\n",
    "\n",
    "sigma_err = .05\n",
    "FV = sigma_err**2 / (0.2**2 + sigma_err**2) * 100\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"$\\\\sigma_\\\\varepsilon={sigma_err}$, FV$={FV:.0f}$%\")\n",
    "\n",
    "ax.axhline(0.2, ls=\"--\", color=\"k\", lw=.5)\n",
    "ax.legend()\n",
    "ax.set(xlim=[None, N], ylabel=\"output standard deviation\", xlabel=\"sample size\", title=f\"# params=1\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd3a97",
   "metadata": {},
   "source": [
    "### Convergence with number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044edfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "mu = 5\n",
    "std = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4643281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, sigma_err=1):\n",
    "    return X @ np.array([1] * X.shape[1]) + sigma_err * np.random.randn(X.shape[0])\n",
    "\n",
    "def g(x, sigma_err=1):\n",
    "    return X @ np.array([1.01] * X.shape[1]) + sigma_err * np.random.randn(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7448ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "fig, ax = plt.subplots(figsize=(8, 4), dpi=100)\n",
    "\n",
    "sigma_err = .1\n",
    "FV = sigma_err**2 / (np.sqrt(0.01**2 * std**2 * M)**2 + sigma_err**2) * 100\n",
    "VR = sigma_err / np.sqrt(0.01**2 * std**2 * M)\n",
    "\n",
    "M = 10000\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "M = 1000\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "M = 100\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "M = 10\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlim=[None, N], ylabel=\"output standard deviation\", xlabel=\"sample size\",\n",
    "       title=f\"$\\\\sigma_\\\\varepsilon={sigma_err}$, FV$={FV:.0f}$%, \" + \"$\\\\sqrt{\\\\frac{V_{MC}}{V_{ND}}}=$\" + f\"${VR:.1f}$\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "fig, ax = plt.subplots(figsize=(8, 4), dpi=100)\n",
    "\n",
    "sigma_err = .3\n",
    "FV = sigma_err**2 / (np.sqrt(0.01**2 * std**2 * M)**2 + sigma_err**2) * 100\n",
    "VR = sigma_err / np.sqrt(0.01**2 * std**2 * M)\n",
    "\n",
    "M = 10000\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "M = 1000\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "M = 100\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "M = 10\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlim=[None, N], ylabel=\"output standard deviation\", xlabel=\"sample size\",\n",
    "       title=f\"$\\\\sigma_\\\\varepsilon={sigma_err}$, FV$={FV:.0f}$%, \" + \"$\\\\sqrt{\\\\frac{V_{MC}}{V_{ND}}}=$\" + f\"${VR:.1f}$\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "fig, ax = plt.subplots(figsize=(8, 4), dpi=100)\n",
    "\n",
    "sigma_err = .4\n",
    "FV = sigma_err**2 / (np.sqrt(0.01**2 * std**2 * M)**2 + sigma_err**2) * 100\n",
    "VR = sigma_err / np.sqrt(0.01**2 * std**2 * M)\n",
    "\n",
    "M = 10000\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "M = 1000\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "M = 100\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "M = 10\n",
    "X = std * np.random.randn(N, M) + mu\n",
    "Y1 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "Y2 = g(X, sigma_err=sigma_err) - f(X, sigma_err=sigma_err)\n",
    "s = pd.Series({nsmp: np.sqrt(np.cov(Y1[:nsmp], Y2[:nsmp]))[1, 0] for nsmp in range(2, N + 1)})\n",
    "pd.Series(s).plot(ax=ax, logx=True, label=f\"# params={M}\")\n",
    "ax.axhline(np.sqrt(0.01**2 * std**2 * M), ls=\"--\", color=\"k\", lw=.5)\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlim=[None, N], ylabel=\"output standard deviation\", xlabel=\"sample size\",\n",
    "       title=f\"$\\\\sigma_\\\\varepsilon={sigma_err}$, FV$={FV:.0f}$%, \" + \"$\\\\sqrt{\\\\frac{V_{MC}}{V_{ND}}}=$\" + f\"${VR:.1f}$\")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sandy-v1.0] *",
   "language": "python",
   "name": "conda-env-sandy-v1.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
